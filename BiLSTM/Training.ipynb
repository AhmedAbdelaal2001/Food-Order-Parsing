{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the Word2Vec model (binary format)\n",
    "model_path = \"C:\\\\Users\\\\Marwan\\\\Desktop\\\\College\\\\5_1\\\\NLP\\\\Food-Order-Parsing\\\\word2vec\\\\GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the model (if not already cached) and load it\n",
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "path = lambda x: f\"datasets/LSTM_{x}.json\"\n",
    "X_train, y_train = load_lstm_dataset_from_json(path(\"train_12000\"))\n",
    "X_dev, y_dev = load_lstm_dataset_from_json(path(\"dev\"))\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Dev size:\", len(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, TimeDistributed, Masking\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "# Function to get Word2Vec vectors for a sentence\n",
    "def sentence_to_embeddings(sentence, word2vec, embedding_dim):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        if word in word2vec:\n",
    "            embeddings.append(word2vec[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))  # OOV words get a zero vector\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Convert sentences to Word2Vec embeddings\n",
    "X_train_embeddings = [sentence_to_embeddings(sentence, word2vec, embedding_dim) for sentence in X_train]\n",
    "X_dev_embeddings = [sentence_to_embeddings(sentence, word2vec, embedding_dim) for sentence in X_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = lambda x: f\"datasets/LSTM_{x}.json\"\n",
    "X_train, y_train = load_lstm_dataset_from_json(path(\"train\"), size=12000)\n",
    "X_dev, y_dev = load_lstm_dataset_from_json(path(\"dev\"))\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Dev size:\", len(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_len = max(\n",
    "    max(len(seq) for seq in X_train_embeddings),\n",
    "    max(len(seq) for seq in X_dev_embeddings)\n",
    ")\n",
    "X_train_padded = pad_sequences(X_train_embeddings, maxlen=max_len, dtype='float32', padding='post')\n",
    "X_dev_padded = pad_sequences(X_dev_embeddings, maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "# Pad and one-hot encode the labels\n",
    "y_train_padded = pad_sequences(y_train, maxlen=max_len, padding='post', value=-1)\n",
    "y_dev_padded = pad_sequences(y_dev, maxlen=max_len, padding='post', value=-1)\n",
    "\n",
    "num_classes = len(set(label for seq in y_train + y_dev for label in seq))\n",
    "\n",
    "y_train_one_hot = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_train_padded])\n",
    "y_dev_one_hot = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_dev_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable Eager Execution if not enabled already\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# ... (Your existing code)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(max_len, embedding_dim))),  # BiLSTM layer\n",
    "    TimeDistributed(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "])\n",
    "\n",
    "# Get the mask from the input\n",
    "mask = tf.not_equal(X_train_padded, 0.0) # Assuming 0.0 is your padding value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    X_train_padded,\n",
    "    y_train_one_hot,\n",
    "    validation_data=(X_dev_padded, y_dev_one_hot),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('my_lstm_model.keras')\n",
    "\n",
    "# Assuming 'new_sentence' is your input sentence\n",
    "new_sentence_embeddings = sentence_to_embeddings(new_sentence, word2vec, embedding_dim)\n",
    "new_sentence_padded = pad_sequences([new_sentence_embeddings], maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "predictions = loaded_model.predict(new_sentence_padded)\n",
    "predicted_labels = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_with_tolerance(model, X_dev, y_dev, word2vec, embedding_dim, max_len, tolerance=2):\n",
    "    \"\"\"\n",
    "    Calculates accuracy with a tolerance for incorrect elements.\n",
    "\n",
    "    Args:\n",
    "        model: The trained LSTM model.\n",
    "        X_dev: The original unpadded input data for the development set.\n",
    "        y_dev: The true labels for the development set.\n",
    "        word2vec: The Word2Vec model used for embeddings.\n",
    "        embedding_dim: The embedding dimension.\n",
    "        max_len: The maximum sequence length used for padding during training.\n",
    "        tolerance: The maximum number of incorrect elements allowed for a sentence to be considered correct.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy score.\n",
    "    \"\"\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i in range(len(X_dev)):\n",
    "        sentence = X_dev[i]\n",
    "        true_labels = y_dev[i]\n",
    "\n",
    "        # Get embeddings for the current sentence\n",
    "        sentence_embeddings = sentence_to_embeddings(sentence, word2vec, embedding_dim)\n",
    "\n",
    "        # Pad the sentence embeddings to match the model's input shape\n",
    "        sentence_padded = pad_sequences([sentence_embeddings], maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "        # Get predictions for the padded sentence\n",
    "        predictions = model.predict(sentence_padded)\n",
    "        predicted_labels = np.argmax(predictions, axis=-1)[0]  # Get predicted labels for the sentence\n",
    "\n",
    "        # Calculate the number of incorrect elements\n",
    "        num_incorrect = np.sum(predicted_labels[:len(true_labels)] != true_labels)\n",
    "\n",
    "        # Check if the number of incorrect elements is within the tolerance\n",
    "        if num_incorrect <= tolerance:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Example usage with tolerance=2:\n",
    "accuracy = calculate_accuracy_with_tolerance(model, X_dev, y_dev, word2vec, embedding_dim, max_len, tolerance=2)\n",
    "print(f\"Accuracy with tolerance 5: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
