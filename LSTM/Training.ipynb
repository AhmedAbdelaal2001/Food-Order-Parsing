{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMu2C5eZSg5h",
        "outputId": "f1b24de8-d708-4be1-cac9-2882a44ae0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download the model (if not already cached) and load it\n",
        "word2vec = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG-ieIljStv_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def load_lstm_dataset_from_json(file_path: str, size=None, start=0):\n",
        "    X, y = [], []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            if i < start:\n",
        "                continue\n",
        "            if size is not None and i >= start + size:\n",
        "                break\n",
        "            try:\n",
        "                sample = json.loads(line)\n",
        "                X.append(sample[\"train.SRC\"])  # Ensure the key exists\n",
        "                y.append(sample[\"train.LABELS\"])\n",
        "            except KeyError as e:\n",
        "                print(f\"KeyError at line {i+1}: {e}\")\n",
        "                print(f\"Line content: {line}\")\n",
        "                raise\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSONDecodeError at line {i+1}: {e}\")\n",
        "                print(f\"Line content: {line}\")\n",
        "                raise\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpM_0VH1Tdxc"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, y_train = load_lstm_dataset_from_json(\"/content/segmented_PIZZA_train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYL-aG-NTgF4"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = load_lstm_dataset_from_json(\"/content/segmented_PIZZA_test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kpyj35osTwPQ"
      },
      "outputs": [],
      "source": [
        "def classify_with_average_embedding(candidate,avg):\n",
        "    similarity = 0\n",
        "    words = candidate.split()  # Split the candidate into words\n",
        "    for word in words:\n",
        "        word_embeddings = [np.zeros(300)]\n",
        "        if word in word2vec:\n",
        "          word_embeddings =  [word2vec[word]]\n",
        "    if word_embeddings:\n",
        "        candidate_embedding = np.mean(word_embeddings, axis=0)\n",
        "        similarity = cosine_similarity([avg], [candidate_embedding])[0][0]\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BrNnbJJdUKYT"
      },
      "outputs": [],
      "source": [
        "def calculate_average_embedding(topping_list):\n",
        "    embeddings = []\n",
        "    for topping in topping_list:\n",
        "        words = topping.split()  # Split topping into words\n",
        "        for word in words:\n",
        "            if word in word2vec:\n",
        "              embeddings.append(word2vec[word])\n",
        "\n",
        "    # Calculate the average embedding for all toppings\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u2Hsq9lXUVmk"
      },
      "outputs": [],
      "source": [
        "styles_list = [\n",
        "    \"cauliflower crust\", \"cauliflower crusts\", \"gluten free crust\", \"gluten-free crust\",\n",
        "    \"gluten free crusts\", \"gluten-free crusts\", \"keto crust\", \"keto crusts\",\n",
        "    \"sourdough crust\", \"sourdough crusts\", \"stuffed crust\", \"stuffed crusts\",\n",
        "    \"thick crust\", \"thick crusts\", \"high rise dough\", \"thin crust\", \"thin crusts\",\n",
        "    \"vegan\", \"vegetarian\", \"veggie\", \"supreme\", \"new york style\", \"big new yorker\",\n",
        "    \"napolitana\", \"napolitan\", \"neapolitan\", \"mediterranean\", \"med\", \"mexican\",\n",
        "    \"big meat\", \"meat lover\", \"meat lovers\", \"meatlover\", \"meatlovers\", \"every meat\",\n",
        "    \"all meat\", \"margherita\", \"margarita\", \"hawaiian\", \"deep dish\", \"deepdish\",\n",
        "    \"pan\", \"combination\", \"chicago style\", \"chicago\", \"all the cheese\", \"all cheese\",\n",
        "    \"cheese lover\", \"cheese lovers\", \"all the toppings\", \"everything\", \"with the works\",\n",
        "    \"every topping\", \"all the vegetables\", \"all veggies\"\n",
        "]\n",
        "\n",
        "styles_avg = calculate_average_embedding(styles_list)\n",
        "\n",
        "def is_style(sentence, i):\n",
        "    window_size =3\n",
        "    for j in range(-window_size, window_size + 1):  # Check from -3 to +3 words\n",
        "        if j != 0:  # Skip the current word itself\n",
        "            new_i = i + j\n",
        "            if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "                if phrase in styles_list:\n",
        "                    return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],styles_avg)\n",
        "    return ret_val\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NA2V7qf2UYbJ"
      },
      "outputs": [],
      "source": [
        "sizes_list = [\n",
        "    \"small\", \"medium\", \"large\", \"extra large\", \"regular\", \"party size\",\n",
        "    \"party sized\", \"party - sized\", \"party - size\", \"lunch size\",\n",
        "    \"lunch sized\", \"lunch - sized\", \"lunch - size\", \"personal size\",\n",
        "    \"personal\", \"personal sized\", \"personal - sized\", \"personal - size\"\n",
        "]\n",
        "sizes_avg = calculate_average_embedding(sizes_list)\n",
        "\n",
        "def is_size(sentence, i):\n",
        "    for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "              # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                    if phrase in sizes_list:  # Check if the phrase is in sizes_list\n",
        "                        return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],sizes_avg)\n",
        "    return ret_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_KnoaOAwUagc"
      },
      "outputs": [],
      "source": [
        "drinks_list = [\n",
        "    \"7 up\", \"7 ups\", \"seven up\", \"seven ups\", \"cherry coke\", \"cherry cokes\",\n",
        "    \"cherry pepsi\", \"cherry pepsis\", \"coffee\", \"coffees\", \"coke\", \"cokes\",\n",
        "    \"coke zero\", \"coke zeros\", \"coke zeroes\", \"dr pepper\", \"dr peppers\",\n",
        "    \"dr peper\", \"dr pepers\", \"doctor peppers\", \"doctor pepper\", \"doctor pepers\",\n",
        "    \"doctor peper\", \"fanta\", \"fantas\", \"ginger ale\", \"ginger ales\",\n",
        "    \"ice tea\", \"iced tea\", \"ice teas\", \"iced teas\", \"lemon ice tea\",\n",
        "    \"lemon iced tea\", \"lemon ice teas\", \"lemon iced teas\", \"mountain dew\",\n",
        "    \"mountain dews\", \"pellegrino\", \"pellegrinos\", \"san pellegrino\",\n",
        "    \"san pellegrinos\", \"pepsi\", \"pepsis\", \"perrier\", \"perriers\",\n",
        "    \"pineapple soda\", \"pineapple sodas\", \"sprite\", \"sprites\", \"water\",\n",
        "    \"waters\", \"diet pepsi\", \"diet pepsis\", \"diet coke\", \"diet cokes\",\n",
        "    \"diet sprite\", \"diet sprites\", \"diet ice tea\", \"diet iced tea\",\n",
        "    \"diet ice teas\", \"diet iced teas\"\n",
        "]\n",
        "drinks_avg = calculate_average_embedding(drinks_list)\n",
        "\n",
        "def is_drink(sentence, i):\n",
        "    for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "              # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                    if phrase in drinks_list:  # Check if the phrase is in sizes_list\n",
        "                        return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],drinks_avg)\n",
        "    return ret_val\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CsWfB6kpUcJC"
      },
      "outputs": [],
      "source": [
        "quantities_list = [\n",
        "    \"light\", \"go light on the\", \"go light on\", \"light on the\", \"light on\",\n",
        "    \"little\", \"a little\", \"just a little\", \"just a bit\", \"only a little\",\n",
        "    \"only a bit\", \"not a lot of\", \"not a lot\", \"not much\", \"not many\",\n",
        "    \"a little bit\", \"a little bit of\", \"a drizzle of\", \"a drizzle\",\n",
        "    \"just a drizzle\", \"just a drizzle of\", \"only a drizzle\",\n",
        "    \"only a drizzle of\", \"no more than a drizzle\", \"no more than a drizzle of\",\n",
        "    \"just a tiny bit of\", \"a tiny bit of\", \"go heavy on\", \"go heavy on the\",\n",
        "    \"heavy on\", \"heavy on the\", \"lots of\", \"a lot of\", \"a whole lot of\",\n",
        "    \"a bunch of\", \"a whole bunch of\", \"extra\", \"lot of\",\"lot\"\n",
        "]\n",
        "quantities_avg = calculate_average_embedding(quantities_list)\n",
        "\n",
        "def is_quantity(sentence, i):\n",
        "    for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "              # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "                    if phrase in quantities_list:  # Check if the phrase is in sizes_list\n",
        "                        return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],quantities_avg)\n",
        "    return ret_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uCUjS890vm-"
      },
      "outputs": [],
      "source": [
        "is_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gaPmI5WtUd1P"
      },
      "outputs": [],
      "source": [
        "toppings_list = [\n",
        "    \"alfredo chicken\", \"american cheese\", \"anchovy\", \"anchovies\", \"artichoke\", \"artichokes\",\n",
        "    \"arugula\", \"bacon\", \"bacons\", \"apple wood bacon\", \"applewood bacon\", \"balsamic glaze\",\n",
        "    \"balzamic glaze\", \"banana pepper\", \"banana peppers\", \"basil\", \"bay leaves\", \"bbq chicken\",\n",
        "    \"barbecue chicken\", \"bbq pulled pork\", \"barbecue pulled pork\", \"bbq sauce\", \"barbecue sauce\",\n",
        "    \"bean\", \"beans\", \"beef\", \"ground beef\", \"broccoli\", \"brocoli\", \"buffalo chicken\", \"buffalo mozzarella\",\n",
        "    \"buffalo mozarella\", \"buffalo sauce\", \"caramelized onions\", \"caramelized red onions\", \"caramelized onion\",\n",
        "    \"caramelized red onion\", \"carrot\", \"carrots\", \"cheddar cheese\", \"cheese\", \"cheeseburger\", \"cherry tomato\",\n",
        "    \"cherry tomatoes\", \"chicken\", \"chickens\", \"chorizo\", \"chorrizo\", \"cumin\", \"dried pepper\", \"dried peppers\",\n",
        "    \"dried tomato\", \"dried tomatoes\", \"feta cheese\", \"feta\", \"fried onion\", \"fried onions\", \"garlic\",\n",
        "    \"garlic powder\", \"green olive\", \"green olives\", \"green pepper\", \"green peppers\", \"grilled chicken\",\n",
        "    \"grilled pineapple\", \"ham\", \"hams\", \"hot pepper\", \"hot peppers\", \"italian sausage\", \"jalapeno pepper\",\n",
        "    \"jalapeno\", \"jalapeno peppers\", \"jalapenos\", \"kalamata olive\", \"kalamata olives\", \"lettuce\",\n",
        "    \"low fat cheese\", \"meatball\", \"meatballs\", \"mozzarella cheese\", \"mozarella cheese\", \"mozzarella\",\n",
        "    \"mozarella\", \"mushroom\", \"mushrooms\", \"olive oil\", \"olives\", \"olive\", \"black olive\", \"black olives\",\n",
        "    \"onions\", \"onion\", \"oregano\", \"parmesan cheese\", \"parmesan\", \"parsley\", \"pea\", \"peas\", \"pecorino cheese\",\n",
        "    \"pecorino\", \"pepperoni\", \"peppperoni\", \"pepperonis\", \"peppperonis\", \"peperoni\", \"peperonis\",\n",
        "    \"peperroni\", \"peperonni\", \"peperronni\", \"peppers\", \"pepper\", \"pesto\", \"pestos\", \"pesto sauce\",\n",
        "    \"pickle\", \"pickles\", \"pineapple\", \"pineapples\", \"pineaple\", \"pineaples\", \"ranch sauce\", \"red onion\",\n",
        "    \"red onions\", \"red pepper flake\", \"red pepper flakes\", \"red peppers\", \"red pepper\", \"ricotta cheese\",\n",
        "    \"ricotta\", \"roasted chicken\", \"roasted garlic\", \"roasted pepper\", \"roasted peppers\", \"roasted red pepper\",\n",
        "    \"roasted red peppers\", \"roasted green pepper\", \"roasted green peppers\", \"roasted tomato\",\n",
        "    \"roasted tomatoes\", \"rosemary\", \"salami\", \"sauce\", \"sausage\", \"sausages\", \"shrimp\", \"shrimps\",\n",
        "    \"spiced sausage\", \"spicy red sauce\", \"spinach\", \"tomato sauce\", \"tomato\", \"tomatoes\", \"tuna\", \"tunas\",\n",
        "    \"vegan pepperoni\", \"white onion\", \"white onions\", \"yellow pepper\", \"yellow peppers\",\"green\",\"red\",\"oil\"\n",
        "]\n",
        "\n",
        "toppings_avg = calculate_average_embedding(toppings_list)\n",
        "\n",
        "def is_topping(sentence, i):\n",
        "\n",
        "    for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "              # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                    if phrase in toppings_list:  # Check if the phrase is in sizes_list\n",
        "                        return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],toppings_avg)\n",
        "    return ret_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n0BXXSOZUgMI"
      },
      "outputs": [],
      "source": [
        "numbers_list = [\"1\",\"a pizza\",\"a drink\", \"a small\",\"a medium\", \"a large\", \"one\", \"just one\", \"only one\", \"two\", \"2\", \"three\", \"3\", \"four\", \"4\", \"five\", \"5\", \"six\", \"6\", \"seven\", \"7\", \"eight\", \"8\", \"nine\", \"9\", \"ten\", \"10\", \"eleven\", \"11\", \"twelve\", \"12\", \"thirteen\", \"13\", \"fourteen\", \"14\", \"fifteen\", \"15\"]\n",
        "numbers_avg = calculate_average_embedding(numbers_list)\n",
        "\n",
        "def is_number(sentence, i):\n",
        "\n",
        "    for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "              # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                    if phrase in numbers_list:  # Check if the phrase is in sizes_list\n",
        "                        return 1\n",
        "    ret_val = classify_with_average_embedding(sentence[i],numbers_avg)\n",
        "    return ret_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yl59_iayUiGb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def word2features(sentence, i):\n",
        "    sentence[i] = sentence[i].lower()\n",
        "    features = [0,0,0,0,0,0]\n",
        "    # Initialize the features for each category\n",
        "    style = is_style(sentence, i)\n",
        "    if style == 1:\n",
        "      features[0] = 1\n",
        "      return np.array(features)\n",
        "\n",
        "    size = is_size(sentence, i)\n",
        "    if size == 1:\n",
        "      features[1] = 1\n",
        "      return np.array(features)\n",
        "\n",
        "\n",
        "    drink = is_drink(sentence, i)\n",
        "    if drink == 1:\n",
        "      features[2] = 1\n",
        "      return np.array(features)\n",
        "\n",
        "\n",
        "    quantity = is_quantity(sentence, i)\n",
        "    if quantity ==1:\n",
        "      features[3] = 1\n",
        "      return np.array(features)\n",
        "\n",
        "    topping = is_topping(sentence, i)\n",
        "    if topping == 1:\n",
        "      features[4] = 1\n",
        "      return np.array(features)\n",
        "\n",
        "    number = is_number(sentence, i)\n",
        "    if number == 1:\n",
        "      features[5] =1\n",
        "      return np.array(features)\n",
        "    # Store all feature values in a list\n",
        "    temp_features = [style, size, drink, quantity, topping, number]\n",
        "\n",
        "    # If no feature is set to 1, choose the max feature (if > 0.5)\n",
        "\n",
        "    max_value = max(temp_features)\n",
        "    if max_value > 0.51:\n",
        "        features[temp_features.index(max_value)] = 1  # Set the feature with max value to 1\n",
        "\n",
        "    return np.array(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WPEL14U5Ukys"
      },
      "outputs": [],
      "source": [
        "def sentence_to_embeddings(sentence):\n",
        "    embeddings = []\n",
        "\n",
        "    for index, word in enumerate(sentence.split()):\n",
        "        categories_features = word2features(sentence.split(),index)\n",
        "\n",
        "        if word in word2vec:\n",
        "          features = word2vec[sentence.split()[index]]\n",
        "        else:\n",
        "          features = np.zeros(300)\n",
        "\n",
        "        embeddings.append(np.concatenate((features,np.array(categories_features))))\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JD_w38V2WDa2"
      },
      "outputs": [],
      "source": [
        "res = sentence_to_embeddings(\"thin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_V2H5fP6upY",
        "outputId": "40693dd6-040e-468b-bf46-8bcf04d8d3c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.03881836,  0.24804688,  0.02734375, -0.03515625, -0.18261719,\n",
              "         0.17089844, -0.06030273, -0.0213623 ,  0.06396484,  0.11914062,\n",
              "        -0.25      ,  0.35742188, -0.01733398, -0.11230469, -0.10595703,\n",
              "        -0.01556396,  0.11425781,  0.265625  , -0.21777344,  0.00567627,\n",
              "        -0.04663086, -0.24609375, -0.2578125 ,  0.07373047,  0.19824219,\n",
              "        -0.14941406, -0.20410156, -0.02050781,  0.1171875 , -0.22753906,\n",
              "         0.11376953,  0.2890625 ,  0.39648438, -0.19238281, -0.13085938,\n",
              "         0.07763672, -0.13867188,  0.02124023,  0.15429688, -0.08740234,\n",
              "         0.27734375, -0.48242188, -0.00860596, -0.29492188,  0.10986328,\n",
              "        -0.20605469, -0.48242188,  0.03027344,  0.31054688, -0.13769531,\n",
              "         0.06396484, -0.23144531, -0.22363281,  0.42382812,  0.00314331,\n",
              "        -0.14746094,  0.06103516, -0.09863281, -0.23632812, -0.08886719,\n",
              "         0.01068115,  0.27539062, -0.10107422, -0.04150391,  0.10839844,\n",
              "        -0.01147461, -0.15234375,  0.23828125,  0.24902344,  0.04296875,\n",
              "        -0.00704956,  0.15820312, -0.10498047,  0.04345703, -0.3359375 ,\n",
              "        -0.13671875,  0.11572266, -0.03881836,  0.07080078,  0.24414062,\n",
              "         0.13671875, -0.14746094, -0.08496094, -0.00927734, -0.109375  ,\n",
              "        -0.06738281, -0.29101562,  0.43554688,  0.19335938,  0.20703125,\n",
              "        -0.12011719,  0.23339844, -0.11035156,  0.09277344, -0.09716797,\n",
              "        -0.02612305,  0.11328125,  0.06494141,  0.15136719, -0.18652344,\n",
              "        -0.06884766, -0.07910156,  0.04907227,  0.03759766,  0.125     ,\n",
              "         0.10644531, -0.05932617, -0.17480469,  0.08447266, -0.1328125 ,\n",
              "         0.15332031, -0.04980469,  0.23242188,  0.03442383, -0.03808594,\n",
              "        -0.30859375,  0.03417969,  0.01586914,  0.22949219,  0.11865234,\n",
              "        -0.01000977, -0.21875   ,  0.04956055,  0.1328125 , -0.07666016,\n",
              "        -0.04150391,  0.00393677,  0.00613403,  0.35742188,  0.04663086,\n",
              "         0.33984375,  0.27539062,  0.09472656,  0.28320312, -0.12695312,\n",
              "         0.30859375,  0.08837891, -0.02478027,  0.05444336, -0.14160156,\n",
              "         0.11181641,  0.08056641,  0.18261719, -0.0016098 , -0.19335938,\n",
              "         0.171875  , -0.15039062, -0.28320312,  0.16796875,  0.10205078,\n",
              "        -0.00122833, -0.375     ,  0.22851562,  0.5390625 , -0.28320312,\n",
              "         0.13769531, -0.12402344,  0.01672363, -0.20800781,  0.28320312,\n",
              "        -0.05004883,  0.08056641, -0.01556396, -0.04663086,  0.1953125 ,\n",
              "         0.17382812, -0.14550781, -0.12695312, -0.0703125 ,  0.02355957,\n",
              "        -0.17578125, -0.10693359, -0.02819824,  0.04345703, -0.1796875 ,\n",
              "         0.20117188,  0.11230469, -0.37890625, -0.18164062, -0.03857422,\n",
              "        -0.00747681, -0.38867188, -0.3046875 ,  0.12988281,  0.12207031,\n",
              "        -0.19140625, -0.12890625, -0.21484375,  0.02380371,  0.23730469,\n",
              "         0.04418945, -0.16699219,  0.04589844, -0.2734375 , -0.16699219,\n",
              "         0.18945312, -0.25585938, -0.25390625, -0.02783203, -0.02355957,\n",
              "         0.18066406,  0.05053711,  0.00570679,  0.05200195,  0.05175781,\n",
              "         0.05712891, -0.07177734, -0.1796875 , -0.5       , -0.20898438,\n",
              "        -0.25390625, -0.06542969,  0.1796875 , -0.18066406, -0.15136719,\n",
              "         0.06347656,  0.34960938,  0.109375  , -0.11523438,  0.359375  ,\n",
              "         0.20605469,  0.09570312, -0.20019531,  0.22558594, -0.19335938,\n",
              "        -0.12207031,  0.27539062, -0.20214844,  0.140625  ,  0.09326172,\n",
              "         0.07275391,  0.08447266,  0.29882812,  0.078125  , -0.19042969,\n",
              "        -0.04174805, -0.38867188,  0.11181641, -0.1796875 ,  0.14941406,\n",
              "         0.30078125, -0.11914062, -0.00598145, -0.2890625 ,  0.21191406,\n",
              "        -0.06835938, -0.09765625, -0.19824219, -0.12792969,  0.12402344,\n",
              "        -0.00683594, -0.33398438,  0.23730469,  0.1796875 , -0.02685547,\n",
              "        -0.33398438, -0.05297852,  0.2734375 , -0.17089844,  0.07910156,\n",
              "         0.07177734,  0.15332031, -0.19726562, -0.30664062,  0.05761719,\n",
              "         0.21679688,  0.05786133, -0.34179688, -0.15234375, -0.34960938,\n",
              "         0.08789062,  0.09423828, -0.06640625, -0.00982666,  0.2421875 ,\n",
              "        -0.21582031, -0.08740234, -0.12695312, -0.05883789,  0.12597656,\n",
              "         0.06494141,  0.06298828, -0.09082031,  0.16503906,  0.02490234,\n",
              "        -0.18457031, -0.25390625,  0.15722656,  0.16601562, -0.31445312,\n",
              "        -0.02429199, -0.13964844,  0.16992188, -0.04614258, -0.01782227,\n",
              "        -0.07470703, -0.13867188,  0.01831055, -0.33007812, -0.11572266,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f0gy3tQTh4i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, TimeDistributed, Masking, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "embedding_dim = 306\n",
        "\n",
        "\n",
        "\n",
        "# Convert sentences to Word2Vec embeddings\n",
        "X_train_embeddings = [sentence_to_embeddings(sentence) for sentence in X_train]\n",
        "X_test_embeddings = [sentence_to_embeddings(sentence) for sentence in X_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O1dT-DDW-w_"
      },
      "outputs": [],
      "source": [
        "labels_to_indices = {\"OTHER\":0,\n",
        "                     \"PIZZA_BEGIN\":1,\n",
        "                     \"PIZZA_INTERMEDIATE\":2,\n",
        "                     \"DRINK_BEGIN\":3,\n",
        "                     \"DRINK_INTERMEDIATE\":4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxsbvdBJXCkk"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_train)):\n",
        "  for j in range(len(y_train[i])):\n",
        "    y_train[i][j] = labels_to_indices[y_train[i][j]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alX0i4FJXFC7"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_test)):\n",
        "  for j in range(len(y_test[i])):\n",
        "    y_test[i][j] = labels_to_indices[y_test[i][j]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dwmy-zMXJug"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pad the sequences\n",
        "max_len = max(\n",
        "    max(len(seq) for seq in X_train_embeddings),\n",
        "    max(len(seq) for seq in X_test_embeddings)\n",
        ")\n",
        "X_train_padded = pad_sequences(X_train_embeddings, maxlen=max_len, dtype='float32', padding='post', value=100)\n",
        "X_test_padded = pad_sequences(X_test_embeddings, maxlen=max_len, dtype='float32', padding='post', value=100)\n",
        "\n",
        "\n",
        "y_train_padded = pad_sequences(y_train, maxlen=max_len, padding='post', value=5)\n",
        "y_test_padded = pad_sequences(y_test, maxlen=max_len, padding='post', value=5)\n",
        "\n",
        "num_classes = len(set(label for seq in y_train + y_test for label in seq)) + 1\n",
        "\n",
        "y_train_one_hot = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_train_padded])\n",
        "y_test_one_hot = np.array([to_categorical(seq, num_classes=num_classes) for seq in y_test_padded])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlktIvzFZ1Tf",
        "outputId": "ef7314bf-3146-4ad1-8242-3907aa8ac213"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "lh1TwALwXlFJ",
        "outputId": "219dd95d-1155-48e7-85d7-659a31efb881"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,456,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_18               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">373,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_25                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m600\u001b[0m)             │       \u001b[38;5;34m1,456,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ layer_normalization_18               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m600\u001b[0m)             │           \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m600\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_46 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m373,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_25                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m6\u001b[0m)               │             \u001b[38;5;34m774\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,832,022</span> (6.99 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,832,022\u001b[0m (6.99 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,832,022</span> (6.99 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,832,022\u001b[0m (6.99 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Dropout, TimeDistributed, Dense\n",
        "\n",
        "from tensorflow.keras.layers import LayerNormalization, Bidirectional\n",
        "\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(300, return_sequences=True), input_shape=(max_len, embedding_dim)),\n",
        "    LayerNormalization(),\n",
        "    Dropout(0.3),\n",
        "    LSTM(128, return_sequences=True, recurrent_dropout=0.2),\n",
        "    TimeDistributed(Dense(num_classes, activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwlLhijJXoPE",
        "outputId": "b501216d-52fd-4610-9ff5-1c4217a6c8b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 289ms/step - accuracy: 0.9999 - loss: 2.4180e-04 - val_accuracy: 0.9995 - val_loss: 0.0011\n",
            "Epoch 2/2\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 288ms/step - accuracy: 0.9999 - loss: 2.8850e-04 - val_accuracy: 0.9997 - val_loss: 9.9770e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ea97d8838e0>"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    X_train_padded,\n",
        "    y_train_one_hot,\n",
        "    validation_data=(X_test_padded, y_test_one_hot),\n",
        "    epochs=2,\n",
        "    batch_size=12\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iytnKuQqfEOX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2WBcUKUcrB-"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model(\"/content/lstm_model_final.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ESVfp6NaNly",
        "outputId": "e341da05-0c32-489f-99f8-6431219bfdb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_accuracy(model, X_dev, y_dev, embedding_dim, max_len):\n",
        "    \"\"\"\n",
        "    Calculates accuracy with a tolerance for incorrect elements and creates a dictionary mapping\n",
        "    the number of mistakes to an array of (x, y_pred, y_true).\n",
        "\n",
        "    Args:\n",
        "        model: The trained LSTM model.\n",
        "        X_dev: The original unpadded input data for the development set.\n",
        "        y_dev: The true labels for the development set.\n",
        "        word2vec: The Word2Vec model used for embeddings.\n",
        "        embedding_dim: The embedding dimension.\n",
        "        max_len: The maximum sequence length used for padding during training.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - accuracy: The accuracy score.\n",
        "            - mistake_dict: A dictionary mapping the number of mistakes to an array of (x, y_pred, y_true).\n",
        "    \"\"\"\n",
        "\n",
        "    mistake_dict = {}  # Initialize an empty dictionary to store results\n",
        "    correct_predictions = 0  # Initialize correct prediction counter\n",
        "    total_predictions = 0  # Initialize total prediction counter\n",
        "\n",
        "    for i in range(len(X_dev)):\n",
        "        sentence = X_dev[i]\n",
        "        true_labels = y_dev[i]\n",
        "\n",
        "        # Get embeddings for the current sentence\n",
        "        sentence_embeddings = sentence_to_embeddings(sentence)\n",
        "\n",
        "        # Pad the sentence embeddings to match the model's input shape\n",
        "        sentence_padded = pad_sequences([sentence_embeddings], maxlen=max_len, dtype='float32', padding='post')\n",
        "\n",
        "        # Get predictions for the padded sentence\n",
        "        predictions = model.predict(sentence_padded, verbose=0)\n",
        "        predicted_labels = np.argmax(predictions, axis=-1)[0]  # Get predicted labels for the sentence\n",
        "\n",
        "        # Calculate the number of incorrect elements\n",
        "        num_incorrect = np.sum(predicted_labels[:len(true_labels)] != true_labels)\n",
        "\n",
        "        # Update accuracy counters\n",
        "        total_predictions += 1\n",
        "        if num_incorrect == 0:  # If no mistakes, consider it a correct prediction\n",
        "            correct_predictions += 1\n",
        "            continue\n",
        "\n",
        "        # Add the (x, y_pred, y_true) tuple to the dictionary based on the number of mistakes\n",
        "        if num_incorrect not in mistake_dict:\n",
        "            mistake_dict[num_incorrect] = []\n",
        "        mistake_dict[num_incorrect].append((sentence, predicted_labels[:len(true_labels)], true_labels))\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions  # Calculate accuracy\n",
        "    return accuracy, mistake_dict\n",
        "\n",
        "# Example usage:\n",
        "accuracy, mistake_dict = calculate_accuracy(loaded_model, X_train[5:10], y_train[5:10], embedding_dim, max_len)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLZ9_HKNex9l",
        "outputId": "4c161888-9c13-4ac6-8b66-73710ff6b2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9896831245394252\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
