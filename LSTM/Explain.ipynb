{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56De9pag9xok"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import os\n",
        "\n",
        "\n",
        "class FeaturesExtractor:\n",
        "    def __init__(self,model_path):\n",
        "        self.word2vec_model = self._load_or_download_model(model_path)\n",
        "        ###################################################################################\n",
        "        self.styles_list = [\n",
        "        \"cauliflower crust\", \"cauliflower crusts\", \"gluten free crust\", \"gluten-free crust\",\n",
        "        \"gluten free crusts\", \"gluten-free crusts\", \"keto crust\", \"keto crusts\",\n",
        "        \"sourdough crust\", \"sourdough crusts\", \"stuffed crust\", \"stuffed crusts\",\n",
        "        \"thick crust\", \"thick crusts\", \"high rise dough\", \"thin crust\", \"thin crusts\",\n",
        "        \"vegan\", \"vegetarian\", \"veggie\", \"supreme\", \"new york style\", \"big new yorker\",\n",
        "        \"napolitana\", \"napolitan\", \"neapolitan\", \"mediterranean\", \"med\", \"mexican\",\n",
        "        \"big meat\", \"meat lover\", \"meat lovers\", \"meatlover\", \"meatlovers\", \"every meat\",\n",
        "        \"all meat\", \"margherita\", \"margarita\", \"hawaiian\", \"deep dish\", \"deepdish\",\n",
        "        \"pan\", \"combination\", \"chicago style\", \"chicago\", \"all the cheese\", \"all cheese\",\n",
        "        \"cheese lover\", \"cheese lovers\", \"all the toppings\", \"everything\", \"with the works\",\n",
        "        \"every topping\", \"all the vegetables\", \"all veggies\"\n",
        "        ]\n",
        "\n",
        "        self.styles_avg = self.calculate_average_embedding(self.styles_list)\n",
        "        ####################################################################################\n",
        "        self.sizes_list = [\n",
        "    \"small\", \"medium\", \"large\", \"extra large\", \"regular\", \"party size\",\n",
        "    \"party sized\", \"party - sized\", \"party - size\", \"lunch size\",\n",
        "    \"lunch sized\", \"lunch - sized\", \"lunch - size\", \"personal size\",\n",
        "    \"personal\", \"personal sized\", \"personal - sized\", \"personal - size\"\n",
        "        ]\n",
        "        self.sizes_avg = self.calculate_average_embedding(self.sizes_list)\n",
        "\n",
        "\n",
        "        ####################################################################################\n",
        "        self.drinks_list = [\n",
        "    \"7 up\", \"7 ups\", \"seven up\", \"seven ups\", \"cherry coke\", \"cherry cokes\",\n",
        "    \"cherry pepsi\", \"cherry pepsis\", \"coffee\", \"coffees\", \"coke\", \"cokes\",\n",
        "    \"coke zero\", \"coke zeros\", \"coke zeroes\", \"dr pepper\", \"dr peppers\",\n",
        "    \"dr peper\", \"dr pepers\", \"doctor peppers\", \"doctor pepper\", \"doctor pepers\",\n",
        "    \"doctor peper\", \"fanta\", \"fantas\", \"ginger ale\", \"ginger ales\",\n",
        "    \"ice tea\", \"iced tea\", \"ice teas\", \"iced teas\", \"lemon ice tea\",\n",
        "    \"lemon iced tea\", \"lemon ice teas\", \"lemon iced teas\", \"mountain dew\",\n",
        "    \"mountain dews\", \"pellegrino\", \"pellegrinos\", \"san pellegrino\",\n",
        "    \"san pellegrinos\", \"pepsi\", \"pepsis\", \"perrier\", \"perriers\",\n",
        "    \"pineapple soda\", \"pineapple sodas\", \"sprite\", \"sprites\", \"water\",\n",
        "    \"waters\", \"diet pepsi\", \"diet pepsis\", \"diet coke\", \"diet cokes\",\n",
        "    \"diet sprite\", \"diet sprites\", \"diet ice tea\", \"diet iced tea\",\n",
        "    \"diet ice teas\", \"diet iced teas\"\n",
        "        ]\n",
        "        self.drinks_avg = self.calculate_average_embedding(self.drinks_list)\n",
        "\n",
        "        ######################################################################################\n",
        "\n",
        "        self.quantities_list = [\n",
        "    \"light\", \"go light on the\", \"go light on\", \"light on the\", \"light on\",\n",
        "    \"little\", \"a little\", \"just a little\", \"just a bit\", \"only a little\",\n",
        "    \"only a bit\", \"not a lot of\", \"not a lot\", \"not much\", \"not many\",\n",
        "    \"a little bit\", \"a little bit of\", \"a drizzle of\", \"a drizzle\",\n",
        "    \"just a drizzle\", \"just a drizzle of\", \"only a drizzle\",\n",
        "    \"only a drizzle of\", \"no more than a drizzle\", \"no more than a drizzle of\",\n",
        "    \"just a tiny bit of\", \"a tiny bit of\", \"go heavy on\", \"go heavy on the\",\n",
        "    \"heavy on\", \"heavy on the\", \"lots of\", \"a lot of\", \"a whole lot of\",\n",
        "    \"a bunch of\", \"a whole bunch of\", \"extra\", \"lot of\",\"lot\"\n",
        "        ]\n",
        "        self.quantities_avg = self.calculate_average_embedding(self.quantities_list)\n",
        "\n",
        "        #########################################################################################\n",
        "\n",
        "        self.toppings_list = [\n",
        "    \"alfredo chicken\", \"american cheese\", \"anchovy\", \"anchovies\", \"artichoke\", \"artichokes\",\n",
        "    \"arugula\", \"bacon\", \"bacons\", \"apple wood bacon\", \"applewood bacon\", \"balsamic glaze\",\n",
        "    \"balzamic glaze\", \"banana pepper\", \"banana peppers\", \"basil\", \"bay leaves\", \"bbq chicken\",\n",
        "    \"barbecue chicken\", \"bbq pulled pork\", \"barbecue pulled pork\", \"bbq sauce\", \"barbecue sauce\",\n",
        "    \"bean\", \"beans\", \"beef\", \"ground beef\", \"broccoli\", \"brocoli\", \"buffalo chicken\", \"buffalo mozzarella\",\n",
        "    \"buffalo mozarella\", \"buffalo sauce\", \"caramelized onions\", \"caramelized red onions\", \"caramelized onion\",\n",
        "    \"caramelized red onion\", \"carrot\", \"carrots\", \"cheddar cheese\", \"cheese\", \"cheeseburger\", \"cherry tomato\",\n",
        "    \"cherry tomatoes\", \"chicken\", \"chickens\", \"chorizo\", \"chorrizo\", \"cumin\", \"dried pepper\", \"dried peppers\",\n",
        "    \"dried tomato\", \"dried tomatoes\", \"feta cheese\", \"feta\", \"fried onion\", \"fried onions\", \"garlic\",\n",
        "    \"garlic powder\", \"green olive\", \"green olives\", \"green pepper\", \"green peppers\", \"grilled chicken\",\n",
        "    \"grilled pineapple\", \"ham\", \"hams\", \"hot pepper\", \"hot peppers\", \"italian sausage\", \"jalapeno pepper\",\n",
        "    \"jalapeno\", \"jalapeno peppers\", \"jalapenos\", \"kalamata olive\", \"kalamata olives\", \"lettuce\",\n",
        "    \"low fat cheese\", \"meatball\", \"meatballs\", \"mozzarella cheese\", \"mozarella cheese\", \"mozzarella\",\n",
        "    \"mozarella\", \"mushroom\", \"mushrooms\", \"olive oil\", \"olives\", \"olive\", \"black olive\", \"black olives\",\n",
        "    \"onions\", \"onion\", \"oregano\", \"parmesan cheese\", \"parmesan\", \"parsley\", \"pea\", \"peas\", \"pecorino cheese\",\n",
        "    \"pecorino\", \"pepperoni\", \"peppperoni\", \"pepperonis\", \"peppperonis\", \"peperoni\", \"peperonis\",\n",
        "    \"peperroni\", \"peperonni\", \"peperronni\", \"peppers\", \"pepper\", \"pesto\", \"pestos\", \"pesto sauce\",\n",
        "    \"pickle\", \"pickles\", \"pineapple\", \"pineapples\", \"pineaple\", \"pineaples\", \"ranch sauce\", \"red onion\",\n",
        "    \"red onions\", \"red pepper flake\", \"red pepper flakes\", \"red peppers\", \"red pepper\", \"ricotta cheese\",\n",
        "    \"ricotta\", \"roasted chicken\", \"roasted garlic\", \"roasted pepper\", \"roasted peppers\", \"roasted red pepper\",\n",
        "    \"roasted red peppers\", \"roasted green pepper\", \"roasted green peppers\", \"roasted tomato\",\n",
        "    \"roasted tomatoes\", \"rosemary\", \"salami\", \"sauce\", \"sausage\", \"sausages\", \"shrimp\", \"shrimps\",\n",
        "    \"spiced sausage\", \"spicy red sauce\", \"spinach\", \"tomato sauce\", \"tomato\", \"tomatoes\", \"tuna\", \"tunas\",\n",
        "    \"vegan pepperoni\", \"white onion\", \"white onions\", \"yellow pepper\", \"yellow peppers\",\"green\",\"red\",\"oil\"\n",
        "        ]\n",
        "\n",
        "        self.toppings_avg = self.calculate_average_embedding(self.toppings_list)\n",
        "\n",
        "        ############################################################################################################\n",
        "        self.numbers_list = [\"1\",\"a pizza\",\"a drink\", \"a small\",\"a medium\", \"a large\", \"one\", \"just one\", \"only one\", \"two\", \"2\", \"three\", \"3\", \"four\", \"4\", \"five\", \"5\", \"six\", \"6\", \"seven\", \"7\", \"eight\", \"8\", \"nine\", \"9\", \"ten\", \"10\", \"eleven\", \"11\", \"twelve\", \"12\", \"thirteen\", \"13\", \"fourteen\", \"14\", \"fifteen\", \"15\"]\n",
        "        self.numbers_avg = self.calculate_average_embedding(self.numbers_list)\n",
        "\n",
        "\n",
        "    def calculate_average_embedding(self,topping_list):\n",
        "        embeddings = []\n",
        "        for topping in topping_list:\n",
        "            words = topping.split()  # Split topping into words\n",
        "            for word in words:\n",
        "                if word in self.word2vec_model:\n",
        "                    embeddings.append(self.word2vec_model[word])\n",
        "        # Calculate the average embedding for all toppings\n",
        "        if embeddings:\n",
        "            return np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def classify_with_average_embedding(self, candidate, avg):\n",
        "        similarity = 0\n",
        "        words = candidate.split()  # Split the candidate into words\n",
        "        for word in words:\n",
        "            word_embeddings = [np.zeros(300)]\n",
        "            if word in self.word2vec_model:\n",
        "                word_embeddings =  [self.word2vec_model[word]]\n",
        "        if word_embeddings:\n",
        "            candidate_embedding = np.mean(word_embeddings, axis=0)\n",
        "            similarity = cosine_similarity([avg], [candidate_embedding])[0][0]\n",
        "        return similarity\n",
        "\n",
        "    def is_style(self,sentence, i):\n",
        "        window_size =3\n",
        "        for j in range(-window_size, window_size + 1):  # Check from -3 to +3 words\n",
        "            if j != 0:  # Skip the current word itself\n",
        "                new_i = i + j\n",
        "                if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                    phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "                    if phrase in self.styles_list:\n",
        "                        return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.styles_avg)\n",
        "        return ret_val\n",
        "\n",
        "\n",
        "    def is_size(self,sentence, i):\n",
        "        for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "            for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "                # Skip the current word itself\n",
        "                    new_i = i + j\n",
        "                    if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                        # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                        phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "                        if phrase in self.sizes_list:  # Check if the phrase is in sizes_list\n",
        "                            return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.sizes_avg)\n",
        "        return ret_val\n",
        "\n",
        "\n",
        "    def is_drink(self,sentence, i):\n",
        "        for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "            for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "                # Skip the current word itself\n",
        "                    new_i = i + j\n",
        "                    if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                        # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                        phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                        if phrase in self.drinks_list:  # Check if the phrase is in sizes_list\n",
        "                            return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.drinks_avg)\n",
        "        return ret_val\n",
        "\n",
        "    def is_quantity(self,sentence, i):\n",
        "        for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "            for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "                # Skip the current word itself\n",
        "                    new_i = i + j\n",
        "                    if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                        # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                        phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "                        if phrase in self.quantities_list:  # Check if the phrase is in sizes_list\n",
        "                            return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.quantities_avg)\n",
        "        return ret_val\n",
        "\n",
        "    def is_topping(self,sentence, i):\n",
        "        for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "            for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "                # Skip the current word itself\n",
        "                    new_i = i + j\n",
        "                    if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                        # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                        phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                        if phrase in self.toppings_list:  # Check if the phrase is in sizes_list\n",
        "                            return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.toppings_avg)\n",
        "        return ret_val\n",
        "\n",
        "    def is_number(self,sentence, i):\n",
        "        for window_size in range(0, 4):  # Check window sizes 1, 2, and 3\n",
        "            for j in range(-window_size, window_size + 1):  # Check from -window_size to +window_size words\n",
        "                # Skip the current word itself\n",
        "                    new_i = i + j\n",
        "                    if 0 <= new_i < len(sentence):  # Ensure we don't go out of bounds\n",
        "                        # Construct the phrase by checking words from new_i - window_size to new_i + window_size\n",
        "                        phrase = ' '.join([sentence[k].lower() for k in range(max(0, new_i - window_size), min(len(sentence), new_i + window_size + 1))])\n",
        "\n",
        "                        if phrase in self.numbers_list:  # Check if the phrase is in sizes_list\n",
        "                            return 1\n",
        "        ret_val = self.classify_with_average_embedding(sentence[i],self.numbers_avg)\n",
        "        return ret_val\n",
        "    #######################################################################################\n",
        "\n",
        "\n",
        "    def word2features(self,sentence, i):\n",
        "        sentence[i] = sentence[i].lower()\n",
        "        features = [0,0,0,0,0,0]\n",
        "        # Initialize the features for each category\n",
        "        style = self.is_style(sentence, i)\n",
        "        if style == 1:\n",
        "            features[0] = 1\n",
        "            return np.array(features)\n",
        "\n",
        "        size = self.is_size(sentence, i)\n",
        "        if size == 1:\n",
        "            features[1] = 1\n",
        "            return np.array(features)\n",
        "\n",
        "\n",
        "        drink = self.is_drink(sentence, i)\n",
        "        if drink == 1:\n",
        "            features[2] = 1\n",
        "            return np.array(features)\n",
        "\n",
        "\n",
        "        quantity = self.is_quantity(sentence, i)\n",
        "        if quantity ==1:\n",
        "            features[3] = 1\n",
        "            return np.array(features)\n",
        "\n",
        "        topping = self.is_topping(sentence, i)\n",
        "        if topping == 1:\n",
        "            features[4] = 1\n",
        "            return np.array(features)\n",
        "\n",
        "        number = self.is_number(sentence, i)\n",
        "        if number == 1:\n",
        "            features[5] =1\n",
        "            return np.array(features)\n",
        "        # Store all feature values in a list\n",
        "        temp_features = [style, size, drink, quantity, topping, number]\n",
        "\n",
        "        # If no feature is set to 1, choose the max feature (if > 0.51)\n",
        "        max_value = max(temp_features)\n",
        "        if max_value > 0.51:\n",
        "            features[temp_features.index(max_value)] = 1  # Set the feature with max value to 1\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "\n",
        "    ##########################################################\n",
        "    def sentence_to_embeddings(self,sentence):\n",
        "        embeddings = []\n",
        "        for index, word in enumerate(sentence.split()):\n",
        "            categories_features = self.word2features(sentence.split(),index)\n",
        "\n",
        "            if word in self.word2vec_model:\n",
        "                features = self.word2vec_model[sentence.split()[index]]\n",
        "            else:\n",
        "                features = np.zeros(300)\n",
        "            embeddings.append(np.concatenate((features,np.array(categories_features))))\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    #########################################################\n",
        "    def _load_or_download_model(self,model_path):\n",
        "        if os.path.exists(model_path):\n",
        "            print(\"Loading word2vec model from disk...\")\n",
        "            return KeyedVectors.load(model_path)\n",
        "        else:\n",
        "            print(\"Downloading word2vec model...\")\n",
        "            # Download and save the model\n",
        "            model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "            return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TvoX-PZaGV3Q"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def model_predict(model,sentence,embedder):\n",
        "    indices_to_labels = {\n",
        "        0: \"OTHER\",\n",
        "        1: \"PIZZA_BEGIN\",\n",
        "        2: \"PIZZA_INTERMEDIATE\",\n",
        "        3: \"DRINK_BEGIN\",\n",
        "        4: \"DRINK_INTERMEDIATE\"\n",
        "    }\n",
        "\n",
        "    sentence_embeddings = embedder.sentence_to_embeddings(sentence)\n",
        "    sentence_padded = pad_sequences([sentence_embeddings], dtype='float32', padding='post',value=100)\n",
        "    predictions = model.predict(sentence_padded, verbose=0)\n",
        "    predicted_labels = np.argmax(predictions, axis=-1)[0]\n",
        "    predicted_labels = predicted_labels[:len(sentence.split())]\n",
        "\n",
        "    # Map each predicted integer label to its corresponding string label\n",
        "    mapped_labels = [indices_to_labels[label] for label in predicted_labels]\n",
        "    return mapped_labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8McTSPLGIwud",
        "outputId": "e2425523-58cc-49e7-cd4a-2e50f332527e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading word2vec model from disk...\n",
            "I want a large pizza with pepperoni and a coke\n",
            "['OTHER', 'OTHER', 'PIZZA_BEGIN', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'OTHER', 'DRINK_BEGIN', 'DRINK_INTERMEDIATE']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(\"lstm.keras\")\n",
        "embedder = FeaturesExtractor(\"word2vec/word2vec_model.kv\")\n",
        "sentence = \"I want a large pizza with pepperoni and a coke\"\n",
        "print(sentence)\n",
        "labels = model_predict(loaded_model,sentence,embedder)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading word2vec model from disk...\n",
            "I want a large pizza with pepperoni and a coke\n",
            "['OTHER', 'OTHER', 'PIZZA_BEGIN', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'PIZZA_INTERMEDIATE', 'OTHER', 'DRINK_BEGIN', 'DRINK_INTERMEDIATE']\n"
          ]
        }
      ],
      "source": [
        "from LSTM import LSTM\n",
        "model_path = \"lstm.keras\"\n",
        "word2vec_path = \"word2vec/word2vec_model.kv\"\n",
        "lstm = LSTM(model_path, word2vec_path)\n",
        "sentence = \"I want a large pizza with pepperoni and a coke\"\n",
        "print(sentence)\n",
        "labels = lstm.predict_labels(sentence)\n",
        "print(labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
