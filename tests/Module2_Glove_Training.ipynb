{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hassa\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout ,LayerNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessor import Preprocessor\n",
    "from embeddings import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(\"../dataset/PIZZA_train_sampled.json\", \"../dataset/PIZZA_dev.json\", \"../dataset/PIZZA_test.json\", \n",
    "                            \"../dataset/preprocessed_PIZZA_train.json\", \"../dataset/preprocessed_PIZZA_dev.json\", \n",
    "                            \"../dataset/preprocessed_PIZZA_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove50=\"../dataset/glove.6B.50d.txt\"\n",
    "glove100=\"../dataset/glove.6B.100d.txt\"\n",
    "glove200=\"../dataset/glove.6B.200d.txt\"\n",
    "glove300=\"../dataset/glove.6B.300d.txt\"\n",
    "Glove= Glove(\"../dataset/PIZZA_train_sampled.json\",glove300,300)\n",
    "Glove.save_tokenizer(\"word_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417 18\n",
      "[26 14  3 46 36  7  3 20 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 4  1  1  5 11  1  1  6  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def read_file(doc_path,dataset_type):\n",
    "    with open(doc_path, 'r') as f:\n",
    "            labels=[]\n",
    "            sentences=[]\n",
    "            for line in f:\n",
    "                parsed_line = json.loads(line.strip())\n",
    "                if f\"{dataset_type}.SRC\" in parsed_line:\n",
    "                    sentences.append(parsed_line[f\"{dataset_type}.SRC\"])\n",
    "                if f\"{dataset_type}.LABELS\" in parsed_line:\n",
    "                    labels.append(parsed_line[f\"{dataset_type}.LABELS\"])\n",
    "    return sentences, labels\n",
    "sentences, labels = read_file(\"../dataset/PIZZA_train_sampled.json\",\"train\")\n",
    "\n",
    "# tokenizing words\n",
    "with open('word_tokenizer.pkl', 'rb') as f:\n",
    "    word_tokenizer = pickle.load(f)\n",
    "X_words_sequences = word_tokenizer.texts_to_sequences(sentences)\n",
    "vocab_size = len(word_tokenizer.word_index)+1 # +1 for padding\n",
    "\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "y_label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "\n",
    "with open('label_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(label_tokenizer, f)\n",
    "    \n",
    "max_length = max([len(seq) for seq in X_words_sequences])\n",
    "x_words_padded = pad_sequences(X_words_sequences, maxlen=max_length, padding='post')\n",
    "y_words_padded = pad_sequences(y_label_sequences, maxlen=max_length, padding='post')\n",
    "num_classes_of_labels = len(label_tokenizer.word_index)+1 # +1 for padding\n",
    "print(vocab_size, num_classes_of_labels)\n",
    "print(x_words_padded[0], y_words_padded[0])\n",
    "y_words_cat = to_categorical(y_words_padded, num_classes=num_classes_of_labels)\n",
    "X_word_train = np.array(x_words_padded)\n",
    "y_word_train = np.array(y_words_cat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 43ms/step - accuracy: 0.9769 - loss: 0.0989 - val_accuracy: 0.9301 - val_loss: 0.4141\n",
      "Epoch 2/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 9.1968e-05 - val_accuracy: 0.9328 - val_loss: 0.4670\n",
      "Epoch 3/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.3101e-04 - val_accuracy: 0.9373 - val_loss: 0.4227\n",
      "Epoch 4/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.1017e-05 - val_accuracy: 0.9321 - val_loss: 0.5043\n",
      "Epoch 5/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.7502e-05 - val_accuracy: 0.9370 - val_loss: 0.5204\n",
      "Epoch 6/6\n",
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 5.3654e-06 - val_accuracy: 0.9381 - val_loss: 0.4774\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Build the model for sequence labeling (one label per timestep)\n",
    "embedding_dim=Glove.embedding_dim\n",
    "embedding_matrix=Glove.embedding_matrix\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],  # Load pre-trained GloVe embeddings\n",
    "                    trainable=True)) # Fine-tune the embeddings\n",
    "model.add(Bidirectional(LSTM(units=256, return_sequences=True)))  # return_sequences=True for sequence labeling\n",
    "LayerNormalization() # we use it to normalize the activations of the previous layer at each step\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(TimeDistributed(Dense(num_classes_of_labels, activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_word_train, y_word_train, batch_size=16, epochs=6, validation_split=0.1)\n",
    "model.save('sequence_labeling_glove_model.keras')\n",
    "model_path=\"sequence_labeling_glove_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Accuracy 35.68129330254042\n"
     ]
    }
   ],
   "source": [
    "def predict_labels(sentences, model_path, tokenizer_path, labels_tokenizer_path, max_length):\n",
    "    # Load the pre-trained model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Load tokenizers\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    with open(labels_tokenizer_path, 'rb') as f:\n",
    "        label_tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Tokenize and pad sentences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    X_input = tf.convert_to_tensor(padded_sequences)  # Use TensorFlow tensors for efficiency\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_input, verbose=0)\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Map indices back to labels\n",
    "    index_to_label = {v: k.upper() for k, v in label_tokenizer.word_index.items()}\n",
    "    predicted_labels_mapped = [\n",
    "        [index_to_label.get(idx) for idx in seq if idx != 0] for seq in predicted_labels\n",
    "    ]\n",
    "\n",
    "    return predicted_labels_mapped\n",
    "sentences, true_labels = read_file(\"../dataset/PIZZA_test2.json\",\"test\")\n",
    "tokenizer_path = \"word_tokenizer.pkl\"\n",
    "label_tokenizer_path = \"label_tokenizer.pkl\"\n",
    "max_length = max_length\n",
    "\n",
    "predicted_labels = predict_labels(sentences, model_path, tokenizer_path, label_tokenizer_path, max_length)\n",
    "i=0\n",
    "false_count=0\n",
    "true_count=0\n",
    "for sentence, labels in zip(sentences, predicted_labels):\n",
    "    IsEqual=np.array_equal(labels, true_labels[i])\n",
    "    if(IsEqual==False):\n",
    "        false_count+=1\n",
    "    else:\n",
    "        true_count+=1\n",
    "    i+=1\n",
    "accuracy= true_count/(true_count+false_count)\n",
    "print(\"Sentence Accuracy\",accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
